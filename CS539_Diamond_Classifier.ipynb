{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shihan1714/Diamond-Regression-Model/blob/main/CS539_Diamond_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "LbSRhPJUiOnZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSt1zMOzhug0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Import**"
      ],
      "metadata": {
        "id": "9IOtxW3AiofQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive & Import\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Andrew's Drive Path\n",
        "path ='/content/drive/MyDrive/ECE 539 Project/Data/diamonds.csv'\n",
        "\n",
        "# John's Drive Path\n",
        "# path = '/content/drive/MyDrive/diamonds.csv'\n",
        "\n",
        "# Test load for csv with pandas\n",
        "diamonds_dataset = pd.read_csv(path)\n",
        "rawData = pd.read_csv(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "EG6_SNsEinar",
        "outputId": "2762fc8c-ea37-4a45-8bfe-6a8f7f939499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7792843b0461>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Test load for csv with pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdiamonds_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mrawData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/ECE 539 Project/Data/diamonds.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-Processing**\n",
        "\n"
      ],
      "metadata": {
        "id": "4J1jiy5ylGAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkuR3aBb0oI3",
        "outputId": "ded11a12-628f-4c7b-b5c6-80724598b785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop diamonds where any dimension is equal to 0\n",
        "diamonds_dataset = diamonds_dataset.drop(diamonds_dataset[diamonds_dataset.x == 0].index)\n",
        "diamonds_dataset = diamonds_dataset.drop(diamonds_dataset[diamonds_dataset.y == 0].index)\n",
        "diamonds_dataset = diamonds_dataset.drop(diamonds_dataset[diamonds_dataset.z == 0].index)\n",
        "\n",
        "# Drop missing values for each numeric property\n",
        "diamonds_dataset.dropna(subset=['table'], inplace=True)\n",
        "diamonds_dataset.dropna(subset=['depth'], inplace=True)\n",
        "diamonds_dataset.dropna(subset=['carat'], inplace=True)\n",
        "diamonds_dataset.dropna(subset=['price'], inplace=True)\n",
        "\n",
        "# Drop duplicate values\n",
        "diamonds_dataset = diamonds_dataset.drop_duplicates()\n",
        "\n",
        "# Perform Z-Score normalization for carats\n",
        "z_scores = np.abs(stats.zscore(diamonds_dataset))\n",
        "threshold = 3\n",
        "outliers = (z_scores > threshold).any(axis=1)\n",
        "diamonds_dataset = diamonds_dataset[~outliers]\n"
      ],
      "metadata": {
        "id": "fa6J6-jZkkLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data = preprocess_diamonds_data(rawData)"
      ],
      "metadata": {
        "id": "TFjz5oedPka_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Data into Training & Test**"
      ],
      "metadata": {
        "id": "JQBj5nvpx_yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get x and y cols\n",
        "x = diamonds_dataset.drop(\"price\", axis=1).values\n",
        "y = diamonds_dataset[\"price\"].values\n",
        "\n",
        "# Make train/test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "psiUh2LgmDil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Stacked Regressor**"
      ],
      "metadata": {
        "id": "NdeM9_lk061t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base regression models\n",
        "base_models = [\n",
        "    ('linear_regression', LinearRegression()),\n",
        "    ('xgboost', XGBRegressor(n_estimators=100, random_state=42)),\n",
        "\n",
        "]\n",
        "\n",
        "# Define stacked regressor\n",
        "stacked_regressor = StackingRegressor(estimators=base_models, final_estimator=DecisionTreeRegressor())\n",
        "\n",
        "# Train the stackedr regressor on the data given\n",
        "stacked_regressor.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "d4D7ikVwseB_",
        "outputId": "0df46d84-fb90-446f-a239-51e74a2c949c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-412345364034>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define base regression models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m base_models = [\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'linear_regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'xgboost'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "y6JYQ0V5HgM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Make Sample Predictions Using Stacking Regressor & Evaluate Data**"
      ],
      "metadata": {
        "id": "x90J1_3813yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = stacked_regressor.predict(x_test)\n",
        "\n",
        "# Evaluate the performance of the stacked regressor\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9uPqITA1Q-G",
        "outputId": "c4b06010-de02-4114-c69c-854812bc9846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 366397.44\n",
            "Mean Absolute Error (MAE): 334.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linData = clean_data[['carat', 'x', 'y', 'z']]\n",
        "XGBdata = clean_data.drop(['carat', 'x', 'y', 'z', 'table', 'depth'], axis = 1)\n",
        "X = clean_data.drop('price', axis = 1)\n",
        "y = clean_data['price']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, random_state=0)"
      ],
      "metadata": {
        "id": "jgkYT1CmQTY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define base regression models\n",
        "base_models = [\n",
        "    ('xgboost', XGBRegressor(n_estimators=100, random_state=42)),\n",
        "    ('linear_regression', LinearRegression()),\n",
        "]\n",
        "\n",
        "# Define stacked regressor\n",
        "stacked_regressor = StackingRegressor(estimators=base_models, final_estimator=DecisionTreeRegressor())\n",
        "\n",
        "# Train the stackedr regressor on the data given\n",
        "stacked_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = stacked_regressor.predict(x_test)\n",
        "\n",
        "# Evaluate the performance of the stacked regressor\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLqfSrYcQJ5Z",
        "outputId": "63a79229-1d41-4972-c587-890a67672ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 432735.11\n",
            "Mean Absolute Error (MAE): 360.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linData = clean_data[['carat', 'x', 'y', 'z', 'price']]\n",
        "XGBdata = clean_data.drop(['carat', 'x', 'y', 'z', 'table', 'depth'], axis = 1)\n",
        "X = clean_data.drop('price', axis = 1)\n",
        "y = clean_data['price']\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, random_state=0)\n",
        "\n",
        "# Define base regression models\n",
        "base_models = [\n",
        "    ('linear_regression', LinearRegression()),\n",
        "    ('xgboost', XGBRegressor(n_estimators=100, random_state=42)),\n",
        "]\n",
        "\n",
        "# Define stacked regressor\n",
        "stacked_regressor = StackingRegressor(estimators=base_models, final_estimator=DecisionTreeRegressor())\n",
        "\n",
        "# Train the stackedr regressor on the data given\n",
        "stacked_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = stacked_regressor.predict(x_test)\n",
        "\n",
        "# Evaluate the performance of the stacked regressor\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmQxVRKpRxUO",
        "outputId": "64c84118-ce68-450d-cc9f-22c6f4f8b73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 419523.22\n",
            "Mean Absolute Error (MAE): 355.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_data = preprocess_diamonds_data(rawData, use_one_hot=False)\n",
        "X_big = clean_data.drop('price', axis = 1)\n",
        "y_big = clean_data['price']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_big, y_big, train_size =0.75, random_state = 0)\n",
        "\n",
        "# Assuming you have already split your data and trained individual models\n",
        "linData_train_x = x_train[['carat', 'x', 'y', 'z']]\n",
        "linData_test_x = x_test[['carat', 'x', 'y', 'z']]\n",
        "linData_train_y = y_train\n",
        "XGBdata_train_x = x_train.drop(['carat', 'x', 'y', 'z', 'table', 'depth'], axis = 1)\n",
        "XGBdata_test_x = x_test.drop(['carat', 'x', 'y', 'z', 'table', 'depth'], axis = 1)\n",
        "XGBdata_train_y = y_train"
      ],
      "metadata": {
        "id": "wsqbMe51fp1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lin_model = LinearRegression()\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=100,  # Number of boosting rounds (trees)\n",
        "    learning_rate=0.8,\n",
        "    max_depth=10,       # Maximum depth of each tree\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lin_model.fit(linData_train_x, linData_train_y)\n",
        "xgb_model.fit(XGBdata_train_x, XGBdata_train_y)\n",
        "\n",
        "# Step 2: Make Predictions on Training Set\n",
        "linData_train_preds = lin_model.predict(linData_train_x)\n",
        "XGBdata_train_preds = xgb_model.predict(XGBdata_train_x)\n",
        "\n",
        "# Step 3: Combine Predictions\n",
        "# Here, we're simply stacking the predictions side by side. Ensure alignment with your target variable.\n",
        "com_train_preds = np.column_stack((linData_train_preds, XGBdata_train_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtXmGZFTSt1y",
        "outputId": "85241712-6722-4b9f-f076-690cd4a0ab07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((38589,), (38589,))"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train Final Model\n",
        "final_model = xgb.XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=100,  # Number of boosting rounds (trees)\n",
        "    learning_rate=0.8,\n",
        "    max_depth=10,       # Maximum depth of each tree\n",
        "    random_state=42\n",
        ")\n",
        "final_model.fit(com_train_preds, y_train)  # y_train is your original target variable for training\n",
        "\n",
        "# Step 5: Make Predictions on Test Set\n",
        "linData_test_preds = lin_model.predict(linData_test_x)\n",
        "XGBdata_test_preds = xgb_model.predict(XGBdata_test_x)\n",
        "\n",
        "combined_test_preds = np.column_stack((linData_test_preds, XGBdata_test_preds))\n",
        "\n",
        "# Final prediction\n",
        "final_predictions = final_model.predict(combined_test_preds)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, final_predictions)\n",
        "mae = mean_absolute_error(y_test, final_predictions)\n",
        "print(f'Mean Squared Error: {mse:.2f}')\n",
        "print(f'Mean Absolute Error: {mae:.2f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w9YEW-WckcL",
        "outputId": "4d6fc52b-7a99-491e-f120-24ca180261fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 642991.38\n",
            "Mean Absolute Error: 391.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "\n",
        "def preprocess_diamonds_data(raw_data, apply_encoding=True, use_one_hot=True, standard_scale_xyz=True, z_score_normalize=True):\n",
        "    \"\"\"\n",
        "    Preprocesses the diamonds dataset with options for encoding, standard scaling, and z-score normalization.\n",
        "\n",
        "    Parameters:\n",
        "    - raw_data (pd.DataFrame): The raw DataFrame to preprocess.\n",
        "    - apply_encoding (bool): If True, applies encoding to categorical data (either one-hot or ordinal). Defaults to True.\n",
        "    - use_one_hot (bool): Determines the type of encoding; if True, applies one-hot encoding, otherwise applies ordinal encoding. Ignored if apply_encoding is False. Defaults to True.\n",
        "    - standard_scale_xyz (bool): If True, applies standard scaling to 'x', 'y', 'z' columns. Defaults to True.\n",
        "    - z_score_normalize (bool): If True, applies z-score normalization and removes outliers. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The preprocessed DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    data = raw_data.copy()\n",
        "\n",
        "    # Basic preprocessing\n",
        "    if \"Unnamed: 0\" in data.columns:\n",
        "        data = data.drop([\"Unnamed: 0\"], axis=1)\n",
        "    data = data[(data.x != 0) & (data.y != 0) & (data.z != 0)]\n",
        "    data.dropna(subset=['table', 'depth', 'carat', 'price'], inplace=True)\n",
        "    data.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Encoding categorical data\n",
        "    if apply_encoding:\n",
        "        if use_one_hot:\n",
        "            data = pd.get_dummies(data, columns=['cut', 'color', 'clarity'], prefix=['cut', 'color', 'clarity'])\n",
        "        else:\n",
        "            for column, order in [('clarity', ['IF', 'VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2', 'I1'][::-1]),\n",
        "                                  ('cut', ['Ideal', 'Premium', 'Very Good', 'Good', 'Fair'][::-1]),\n",
        "                                  ('color', sorted(raw_data['color'].unique())[::-1])]:\n",
        "                mapping = {category: i for i, category in enumerate(order)}\n",
        "                data[column] = data[column].map(mapping)\n",
        "\n",
        "    # Standard scaling for 'x', 'y', 'z'\n",
        "    if standard_scale_xyz:\n",
        "        standard_scalar = StandardScaler()\n",
        "        data[['x', 'y', 'z']] = standard_scalar.fit_transform(data[['x', 'y', 'z']])\n",
        "\n",
        "    # Z-score normalization and outlier removal\n",
        "    if z_score_normalize:\n",
        "        z_scores = np.abs(stats.zscore(data.select_dtypes(include=[np.number])))\n",
        "        threshold = 3\n",
        "        data = data[(z_scores < threshold).all(axis=1)]\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "jlhoSwIN2Ax6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}